[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Slides",
    "section": "",
    "text": "Differentiable Optimization\n\n\n\n\n\n\nDavide De Benedittis\n\n\nJun 23, 2025\n\n\n\n\n\nindex.qmd\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#convex-optimization-problem",
    "href": "2025-06-23_differentiable_optimization/index.html#convex-optimization-problem",
    "title": "Differentiable Optimization",
    "section": "Convex Optimization Problem",
    "text": "Convex Optimization Problem\nConsider a parametrized convex optimization problem \\[\n\\begin{aligned}\n& \\min_x \\quad && f(x, \\theta) \\\\\n& \\text{s.t.} && g(x, \\theta) \\leq 0 \\\\\n&&& h(x, \\theta) = 0\n\\end{aligned}\n\\]\nWhere \\(f\\) and \\(g\\) are convex functions, and \\(h\\) is an affine function."
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#kkt-conditions",
    "href": "2025-06-23_differentiable_optimization/index.html#kkt-conditions",
    "title": "Differentiable Optimization",
    "section": "KKT Conditions",
    "text": "KKT Conditions\nKarush-Kuhn-Tucker (KKT) conditions generalize the method of Lagrange multipliers to constrained optimization problems. They provide necessary conditions for a solution to be optimal.\nThe KKT conditions are:\n\n\\[\n\\begin{aligned}\ng(x^*) &\\leq 0, \\qquad &\\text{(Primal feasibility)} \\\\\nh(x^*) &= 0, &\\text{(Primal feasibility)} \\\\\n\\lambda^* &\\geq 0, &\\text{(Dual feasibility)} \\\\\n\\lambda^* \\circ g(x^*) &= 0, &\\text{(Complementary slackness)} \\\\\n\\nabla f(x^*) + (\\lambda^*)^T \\nabla g(x^*) + (\\nu^*)^T \\nabla h(x^*) &= 0 &\\text{(Stationarity)}\n\\end{aligned}\n\\]\n\n\nWhere \\(\\lambda^*\\) and \\(\\nu^*\\) are the Lagrange multipliers for the inequality and equality constraints, respectively. The \\(\\circ\\) operator denotes the element-wise product (Hadamard product)."
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#differentiable-optimization-12",
    "href": "2025-06-23_differentiable_optimization/index.html#differentiable-optimization-12",
    "title": "Differentiable Optimization",
    "section": "Differentiable Optimization (1/2)",
    "text": "Differentiable Optimization (1/2)\nThe gradient of the optimal solution with respect to the parameters \\(\\theta\\) can be computed with\n\nnumerical methods (ðŸ˜¢)\nexplicit methods â€“ write the computational graph that calculates the solution and perform autodiff (ðŸ˜¢)\nimplicit methods â€“ use KKT conditions and implicit function theorem (ðŸ˜„)\n\nIn general, the optimal solution is not available in closed form. However, it is possible to derive the gradient of the optimal solution with respect to the parameters \\(\\theta\\) using the KKT conditions."
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#differentiable-optimization-22",
    "href": "2025-06-23_differentiable_optimization/index.html#differentiable-optimization-22",
    "title": "Differentiable Optimization",
    "section": "Differentiable Optimization (2/2)",
    "text": "Differentiable Optimization (2/2)\nAt the solution, we can treat the solver as simply finding the root of the nonlinear equation set\n\n\\[\nG(x^*, \\lambda^*, \\nu^*) = \\begin{bmatrix}\n\\nabla f(x^*) + (\\lambda^*)^T \\nabla g(x^*) + (\\nu^*)^T \\nabla h(x^*) \\\\\n\\lambda \\circ g(x^*) \\\\\nh(x^*) \\\\\n\\end{bmatrix} = 0\n\\]\n\nUsing the shorthand \\((x^*, \\lambda^*, \\nu^*)(\\theta)\\) as the primal-dual optimal solution as a function of \\(\\theta\\), we have\n\n\\[\n\\begin{split}\n& \\partial_{\\theta} G(x^\\star(\\theta), \\lambda^\\star(\\theta), \\nu^\\star(\\theta), \\theta) = 0 \\\\\n\\Longrightarrow \\;\\; & \\partial_{x, \\lambda, \\nu} G(x^\\star, \\lambda^\\star, \\nu^\\star, \\theta) \\partial_\\theta (x^\\star, \\lambda^\\star, \\nu^\\star)(\\theta) + \\partial_{\\theta} G(x^\\star, \\lambda^\\star, \\nu^\\star, \\theta) = 0 \\\\\n\\Longrightarrow \\;\\; & \\partial_\\theta (x^\\star, \\lambda^\\star, \\nu^\\star)(\\theta) = -\\left (\\partial_{x, \\lambda, \\nu} G(x^\\star, \\lambda^\\star, \\nu^\\star, \\theta) \\right)^{-1} \\partial_{\\theta} G(x^\\star, \\lambda^\\star, \\nu^\\star, \\theta).\n\\end{split}\n\\]\n\nFrom this, we can differentiate through the solution\n\n\\[\n\\partial_{x, \\lambda, \\nu} G(x^*, \\lambda^*, \\nu^*, \\theta) = \\begin{bmatrix}\n\\nabla^2_x f + (\\lambda^*)^T \\nabla^2 _x g & \\partial_x g & \\partial_x h \\\\\n\\partial_x g^T \\operatorname{diag}(\\lambda^*) & \\operatorname{diag}(g) & 0 \\\\\n\\partial_x h^T & 0 & 0 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#optimization-as-a-layer",
    "href": "2025-06-23_differentiable_optimization/index.html#optimization-as-a-layer",
    "title": "Differentiable Optimization",
    "section": "Optimization as a Layer ðŸ§…",
    "text": "Optimization as a Layer ðŸ§…\nThe forward pass computes the optimal solution, the backward pass computes the gradient of the optimal solution with respect to the parameters \\(\\theta\\).\n\nFrom BPQP"
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#applications-1",
    "href": "2025-06-23_differentiable_optimization/index.html#applications-1",
    "title": "Differentiable Optimization",
    "section": "Applications",
    "text": "Applications\n\n\n\n\nLearn hard constraints\nModeling predictions\nGame theory (e.g., Nash equilibrium)\nMeta learning"
  },
  {
    "objectID": "2025-06-23_differentiable_optimization/index.html#applications-2",
    "href": "2025-06-23_differentiable_optimization/index.html#applications-2",
    "title": "Differentiable Optimization",
    "section": "Applications",
    "text": "Applications\n\n\n\nLearn hard constraints\nModeling predictions\nGame theory (e.g., Nash equilibrium)\nMeta learning\nRL and control\n\nOptimization layers in RL\nMPC tuning\nMulti-agent control\nSafety\nHigh dimensional control\n\n\n\n\n\nFrom Actor Critic MPC"
  }
]